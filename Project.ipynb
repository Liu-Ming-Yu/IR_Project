{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884c1d7f-fa5f-49bf-8c18-87ec9954753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Data Loading, Indexing with Whoosh, and Baseline TF-IDF Retrieval\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from whoosh import index\n",
    "from whoosh.fields import Schema, TEXT, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "from whoosh.qparser import MultifieldParser\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d814b2b-e591-4a16-bbc0-b567d22c7985",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Device Setup (for later embedding steps)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "# 2. Load Snippet Dataset\n",
    "def load_snippets(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load code snippet data from a CSV. Expected columns:\n",
    "      - id: unique identifier for the snippet\n",
    "      - language: programming language (e.g., Python, Java)\n",
    "      - snippet: the code text\n",
    "      - description: brief human-readable description\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    required_cols = {'id', 'language', 'snippet', 'description'}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(f\"CSV must contain columns {required_cols}\")\n",
    "    df = df.dropna(subset=['id', 'snippet'])\n",
    "    df['id'] = df['id'].astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# 3. Whoosh Index Creation\n",
    "def create_or_open_index(index_dir: str, schema: Schema) -> index.Index:\n",
    "    \"\"\"\n",
    "    Create a new Whoosh index or open existing one.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.makedirs(index_dir)\n",
    "        ix = index.create_in(index_dir, schema)\n",
    "        print(f\"[INFO] Created new index at {index_dir}\")\n",
    "    else:\n",
    "        ix = index.open_dir(index_dir)\n",
    "        print(f\"[INFO] Opened existing index at {index_dir}\")\n",
    "    return ix\n",
    "\n",
    "# Define schema: id, language, snippet text, description\n",
    "snippet_schema = Schema(\n",
    "    id=ID(stored=True, unique=True),\n",
    "    language=TEXT(stored=True),\n",
    "    snippet=TEXT(stored=True, analyzer=StemmingAnalyzer()),\n",
    "    description=TEXT(stored=True)\n",
    ")\n",
    "\n",
    "# 4. Indexing Function\n",
    "def build_index(df: pd.DataFrame, index_dir: str = \"indexdir\"):\n",
    "    \"\"\"\n",
    "    Build Whoosh index from snippets DataFrame.\n",
    "    \"\"\"\n",
    "    ix = create_or_open_index(index_dir, snippet_schema)\n",
    "    writer = ix.writer()\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        writer.update_document(\n",
    "            id=row['id'],\n",
    "            language=row['language'],\n",
    "            snippet=row['snippet'],\n",
    "            description=row.get('description', '')\n",
    "        )\n",
    "    writer.commit()\n",
    "    print(f\"[INFO] Indexed {len(df)} code snippets.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5. Baseline Retrieval Function\n",
    "def search_snippets(query: str, index_dir: str = \"indexdir\", top_k: int = 10):\n",
    "    \"\"\"\n",
    "    Perform TF-IDF based search over code snippets.\n",
    "    Returns a list of (id, score, language, snippet, description).\n",
    "    \"\"\"\n",
    "    ix = index.open_dir(index_dir)\n",
    "    # Search over `snippet` and `description` fields\n",
    "    parser = MultifieldParser([\"snippet\", \"description\"], schema=ix.schema)\n",
    "    q = parser.parse(query)\n",
    "    with ix.searcher() as searcher:\n",
    "        results = searcher.search(q, limit=top_k)\n",
    "        hits = []\n",
    "        for hit in results:\n",
    "            hits.append({\n",
    "                'id': hit['id'],\n",
    "                'score': hit.score,\n",
    "                'language': hit['language'],\n",
    "                'snippet': hit['snippet'],\n",
    "                'description': hit['description']\n",
    "            })\n",
    "    return hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1f2cab-4eaa-40ec-8b2f-fc4a54984e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Embedding-based Re-ranking with CodeBERT on GPU\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "# 1. Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "# 2. Load snippets and prepare embeddings\n",
    "EMBEDDING_DIR = \"embeddings\"\n",
    "EMBEDDING_FILE = os.path.join(EMBEDDING_DIR, \"snippet_embeddings.npz\")\n",
    "MODEL_NAME = \"microsoft/codebert-base\"\n",
    "\n",
    "# Initialize CodeBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def build_snippet_embeddings(csv_path: str, rebuild: bool = False) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Build or load embeddings for each code snippet.\n",
    "    Returns a dict mapping snippet IDs to embedding vectors.\n",
    "    \"\"\"\n",
    "    os.makedirs(EMBEDDING_DIR, exist_ok=True)\n",
    "    if os.path.exists(EMBEDDING_FILE) and not rebuild:\n",
    "        print(f\"[INFO] Loading existing embeddings from {EMBEDDING_FILE}\")\n",
    "        data = np.load(EMBEDDING_FILE, allow_pickle=True)\n",
    "        ids = data['ids']\n",
    "        embeddings = data['embeddings']\n",
    "        return {id_: emb for id_, emb in zip(ids.tolist(), embeddings)}\n",
    "\n",
    "    print(\"[INFO] Computing new snippet embeddings...\")\n",
    "    df = load_snippets(csv_path)\n",
    "    ids = []\n",
    "    embs = []\n",
    "    batch_size = 16\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(df), batch_size):\n",
    "            batch = df.iloc[i:i+batch_size]\n",
    "            texts = (batch['description'] + \" \" + batch['snippet']).tolist()\n",
    "            # Tokenize\n",
    "            enc = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            # Forward\n",
    "            out = model(**enc)\n",
    "            # Use [CLS] token embedding\n",
    "            cls_emb = out.last_hidden_state[:,0,:]\n",
    "            # Normalize\n",
    "            cls_emb = F.normalize(cls_emb, p=2, dim=1)\n",
    "            # Move to CPU and numpy\n",
    "            batch_embs = cls_emb.cpu().numpy()\n",
    "\n",
    "            for id_, emb in zip(batch['id'], batch_embs):\n",
    "                ids.append(id_)\n",
    "                embs.append(emb)\n",
    "\n",
    "    ids = np.array(ids)\n",
    "    embeddings = np.stack(embs)\n",
    "    np.savez_compressed(EMBEDDING_FILE, ids=ids, embeddings=embeddings)\n",
    "    print(f\"[INFO] Saved embeddings: {EMBEDDING_FILE}\")\n",
    "    return {id_: emb for id_, emb in zip(ids.tolist(), embeddings)}\n",
    "\n",
    "\n",
    "# 3. Re-ranking function\n",
    "def rerank_with_embeddings(\n",
    "    query: str,\n",
    "    csv_path: str,\n",
    "    index_dir: str = \"indexdir\",\n",
    "    top_k: int = 10,\n",
    "    alpha: float = 0.5\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform initial TF-IDF retrieval, then re-rank using combined score:\n",
    "    combined_score = alpha * normalized_tfidf + (1 - alpha) * cosine_similarity\n",
    "\n",
    "    Returns list of hits with added 'cosine' and 'combined_score'.\n",
    "    \"\"\"\n",
    "    # Ensure embeddings are built\n",
    "    embeddings_map = build_snippet_embeddings(csv_path)\n",
    "\n",
    "    # Initial retrieval\n",
    "    hits = search_snippets(query, index_dir=index_dir, top_k=top_k)\n",
    "    if not hits:\n",
    "        return []\n",
    "\n",
    "    # Extract tfidf scores\n",
    "    tfidf_scores = np.array([h['score'] for h in hits], dtype=np.float32)\n",
    "    # Normalize TF-IDF to [0,1]\n",
    "    tfidf_norm = (tfidf_scores - tfidf_scores.min()) / (tfidf_scores.max() - tfidf_scores.min() + 1e-8)\n",
    "\n",
    "    # Embed query\n",
    "    with torch.no_grad():\n",
    "        enc = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        q_out = model(**enc)\n",
    "        q_emb = F.normalize(q_out.last_hidden_state[:,0,:], p=2, dim=1).cpu().numpy()[0]\n",
    "\n",
    "    # Compute cosine similarity with each snippet embedding\n",
    "    cos_sims = []\n",
    "    for h in hits:\n",
    "        emb = embeddings_map.get(h['id'])\n",
    "        if emb is None:\n",
    "            cos = 0.0\n",
    "        else:\n",
    "            cos = float(np.dot(q_emb, emb))\n",
    "        cos_sims.append(cos)\n",
    "    cos_sims = np.array(cos_sims, dtype=np.float32)\n",
    "    # Normalize cosine to [0,1]\n",
    "    cos_norm = (cos_sims - cos_sims.min()) / (cos_sims.max() - cos_sims.min() + 1e-8)\n",
    "\n",
    "    # Compute combined score\n",
    "    combined = alpha * tfidf_norm + (1 - alpha) * cos_norm\n",
    "\n",
    "    # Attach scores and sort\n",
    "    for i, h in enumerate(hits):\n",
    "        h['cosine'] = float(cos_sims[i])\n",
    "        h['combined_score'] = float(combined[i])\n",
    "\n",
    "    hits_sorted = sorted(hits, key=lambda x: x['combined_score'], reverse=True)\n",
    "    return hits_sorted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3973124-88d3-4fb7-a5e4-0e082f0b8825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "\n",
    "# Paths and constants\n",
    "CSV_PATH = \"snippets.csv\"\n",
    "INDEX_DIR = \"indexdir\"\n",
    "\n",
    "# Ensure index is built\n",
    "df = load_snippets(CSV_PATH)\n",
    "build_index(df, INDEX_DIR)\n",
    "\n",
    "# UI Widgets\n",
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type your code query here',\n",
    "    description='Query:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "topk_slider = widgets.IntSlider(\n",
    "    value=5, min=1, max=20, step=1,\n",
    "    description='Top K:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "alpha_slider = widgets.FloatSlider(\n",
    "    value=0.5, min=0.0, max=1.0, step=0.05,\n",
    "    description='Alpha (TF-IDF vs. Embeddings):',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "search_button = widgets.Button(\n",
    "    description='Search',\n",
    "    button_style='primary',\n",
    "    tooltip='Click to run code snippet search',\n",
    "    icon='search'\n",
    ")\n",
    "output = widgets.Output(layout={'border': '1px solid #ddd'})\n",
    "\n",
    "def on_search_click(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        query = query_input.value.strip()\n",
    "        if not query:\n",
    "            display(HTML(\"<p style='color:red;'>Please enter a query.</p>\"))\n",
    "            return\n",
    "        # Retrieve results\n",
    "        results = rerank_with_embeddings(\n",
    "            query=query,\n",
    "            csv_path=CSV_PATH,\n",
    "            index_dir=INDEX_DIR,\n",
    "            top_k=topk_slider.value,\n",
    "            alpha=alpha_slider.value\n",
    "        )\n",
    "        if not results:\n",
    "            display(HTML(\"<p>No results found.</p>\"))\n",
    "            return\n",
    "        \n",
    "        # Build HTML display\n",
    "        html = f\"<h3>Results for '<em>{query}</em>'</h3>\"\n",
    "        for idx, hit in enumerate(results, 1):\n",
    "            # Summarize snippet\n",
    "            summary = \"\"\n",
    "            for line in hit['snippet'].splitlines():\n",
    "                stripped = line.strip()\n",
    "                if stripped.startswith('#') or stripped.startswith('//') or stripped.startswith('def ') or stripped.startswith('function '):\n",
    "                    summary = stripped\n",
    "                    break\n",
    "            if not summary and hit['snippet'].splitlines():\n",
    "                summary = hit['snippet'].splitlines()[0].strip()\n",
    "            \n",
    "            # Highlight keywords\n",
    "            keywords = [t.lower() for t in query.split() if len(t) > 1]\n",
    "            highlighted = \"\"\n",
    "            for line in hit['snippet'].splitlines():\n",
    "                low = line.lower()\n",
    "                if any(k in low for k in keywords):\n",
    "                    highlighted += f\"<mark>{line}</mark>\\n\"\n",
    "                else:\n",
    "                    highlighted += f\"{line}\\n\"\n",
    "            \n",
    "            html += f\"\"\"\n",
    "            <div style=\"border:1px solid #ccc; padding:10px; margin-bottom:10px; border-radius:5px;\">\n",
    "              <strong>{idx}. [ID: {hit['id']}] ({hit['language']})</strong><br>\n",
    "              <small>Score: {hit['combined_score']:.4f} (tfidf), cosine: {hit['cosine']:.4f}</small><br>\n",
    "              <em>Summary:</em> {summary}<br>\n",
    "              <em>Description:</em> {hit.get('description', '')}<br>\n",
    "              <pre style=\"background:#f7f7f7; padding:10px; overflow:auto; white-space:pre-wrap;\">{highlighted}</pre>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        display(HTML(html))\n",
    "\n",
    "search_button.on_click(on_search_click)\n",
    "\n",
    "# Layout and display\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<h2>Code Snippet Retrieval</h2>\"),\n",
    "    query_input,\n",
    "    topk_slider,\n",
    "    alpha_slider,\n",
    "    search_button,\n",
    "    output\n",
    "])\n",
    "\n",
    "display(ui)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5876c2-46ae-4549-9d92-4b6cb3a4e254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ea4331-856e-4c6f-9b8d-ca11fea0fbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
